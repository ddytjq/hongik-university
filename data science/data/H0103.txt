ID=H0103
URL=http://www.klab.caltech.edu/%7Equixote/abstracts.html
SIZE=16976
DATE=11/07/2002
TIME=17:01:05
DATASET=Biology
HTML=<BODY BGCOLOR="#000000" BACKGROUND="backgrnd/olive_paper.gif" 
TEXT="#00FFFF" LINK = "#7FFF00" ALINK = "#FFFF00" VLINK ="#FFFF00">

<H1> Abstracts </H1>

<OL>

<P><li>  
<A NAME="1">
<H4> Neuronal noise sources in membrane patches and linear cables: An 
analytical and experimental study </H4>

The temporal precision  with which neurons respond to  their inputs is
currently an active area of  research since it directly relates to the
nature   of  the   neuronal   code  used   to  transmit   information.
Characterization and  estimation of neuronal noise  sources allows one
to address this  question directly.  Towards this end,  we analyze and
measure the sources  of noise present at the  single neuron level. The
noise sources we consider in  our study include thermal noise, channel
noise   arising   from    the   stochastic   nature   of   microscopic
voltage-dependent  channels  and  synaptic  noise due  to  spontaneous
background activity.   We experimentally measure  these contributions,
focusing in particular on the noise associated with sodium conductance
fluctuations and synaptic  background activity, using whole-cell patch
recordings at the soma of  rat neocortical pyramidal cells. We compare
the empirical power spectra and variances of the noise sources against
those obtained from analytical expressions.
<P>
Using  these   noise  sources  we  derive   measures  quantifying  the
information  loss   of  a  synaptic  signal   as  it  electrotonically
propagates along  a weakly-active dendrite  to the soma. We  model the
dendrite as an infinite linear cable, with the noise sources mentioned
above, distributed  along its length.  This allows  us to analytically
assess  the  role  of  each  of these  noise  sources  in  information
transfer.  More  realistic dendritic geometries will be  analyzed in a
subsequent study.


<P><li>  
<A NAME="2">
<H4> Synaptic Transmission: An Information-theoretic Perspective </H4>

Here  we analyze synaptic  transmission from  an information-theoretic
perspective. We derive closed-form expressions for the lower-bounds on
the  capacity  of a  simple  model of  a  cortical  synapse under  two
explicit coding paradigms.  Under the "signal estimation" paradigm, we
assume the signal  to be encoded in the mean firing  rate of a Poisson
neuron. The performance  of an optimal linear estimator  of the signal
then provides  a lower  bound on the  capacity for  signal estimation.
Under the "signal detection" paradigm,  the presence or absence of the
signal has to be detected.   Performance of the optimal spike detector
allows  us  to  compute a  lower  bound  on  the capacity  for  signal
detection.   We find  that single  synapses (for  empirically measured
parameter   values)  transmit   information  poorly   but  significant
improvement can be achieved with a small amount of redundancy.

<P><li>  
<A NAME="3">
<H4> Signal Detection in Noisy Weakly-Active Dendrites </H4>

Here we derive measures quantifying the information loss of a synaptic
signal  due  to   the  presence  of  neuronal  noise   sources  as  it
electrotonically propagates along a  weakly-active dendrite . We model
the  dendrite  as  an   infinite  linear  cable,  with  noise  sources
distributed  along its  length.   The noise  sources  we consider  are
thermal  noise, channel noise  arising from  the stochastic  nature of
voltage-dependent ionic  channels (potassium and  sodium) and synaptic
noise due to spontaneous  background activity.  We assess the efficacy
of information  transfer using a  signal detection paradigm  where the
objective  is to detect  the presence/absence  of a  presynaptic spike
from  the   post-synaptic  membrane   voltage.   This  allows   us  to
analytically  assess  the role  of  each  of  these noise  sources  in
information transfer.  For our choice  of parameters, we find that the
synaptic noise is  the dominant noise source which  limits the maximum
length over which information be reliably transmitted.


<P><li>  
<A NAME = "4">
<H4> Detecting and estimating signals in noisy cable structures: I. 
Neuronal noise sources </H4>

In  recent theoretical  approaches  addressing the  problem of  neural
coding, tools from statistical  estimation and information theory have
been  applied   to  quantify  the  ability  of   neurons  to  transmit
information  through their  spike outputs.   These  techniques, though
fairly general,  ignore the specific nature of  neuronal processing in
terms of its known biophysical properties. However, a systematic study
of processing at various stages in a biophysically faithful model of a
single  neuron can  identify the  role  of each  stage in  information
transfer. Towards this end, we carry out a theoretical analysis of the
information  loss of  a synaptic  signal propagating  along  a linear,
one-dimensional,  weakly-active cable  due to  neuronal  noise sources
along  the  way, using  both  a  signal  reconstruction and  a  signal
detection paradigm.
<P>
Here we begin such  an analysis by quantitatively characterizing three
sources  of membrane  current  noise.   1. Thermal  noise  due to  the
passive  membrane  resistance, 2.   noise  due  to stochastic  channel
openings and  closings of voltage-gated membrane channels  (Na and K),
and  3. noise  due  to random,  background  synaptic activity.   Using
analytical expressions for the power spectral densities of these noise
sources,  we  compare their  magnitudes  in the  case  of  a patch  of
membrane from  a cortical pyramidal cell and  explore their dependence
on different biophysical parameters.


<P><li>  
<A NAME = "5">
<H4> Detecting and estimating signals in noisy cable structures: II. 
Information-theoretic analysis </H4>

This  is  the  second in  a  series  of  papers attempting  to  recast
classical  single-neuron biophysics in  information-theoretical terms.
Rather than analyzing the voltage or current attenuation suffered by a
synaptic input as it propagates  from the dendritic site of its origin
to the  spike initiation  zone, we analyze  the amount  of information
lost  about the  signal due  to  propagation in  a noisy  environment.
Specifically, using the stochastic,  linear, cable equation we develop
closed-form  expressions  for  the  second-order fluctuations  of  the
membrane potential (and its associated  power spectrum) as a result of
various  membrane current  noise  sources.  Assuming  the presence  of
three  dominant  noise  sources  distributed throughout  the  neuronal
membrane---thermal noise, noise due  to the random opening and closing
of  sodium and potassium  channels and  noise due  to the  presence of
"spontaneous" synaptic  input---we derive closed-form  expressions for
information-theoretic    measures   in    order   to    quantify   the
information-transmission capability of  an infinite linear cable under
two different scenarios.
<P>
In signal estimation, the time-course of the membrane potential at one
location on the cable is  used to reconstruct the detailed time-course
of  a  random,  band-limited  current  injected  some  distance  away.
Estimation performance  is characterized  in terms of  coding fraction
and mutual  information.  In signal detection,  the membrane potential
is used to determine whether  or not a distant synaptic event occurred
within  a   particular  observational  interval.   In   light  of  our
analytical results,  we speculate  that the total  length of  a weakly
active dendrite  might be limited by  the information loss  due to the
accumulated noise between distal synaptic input sites and the soma.

<P><li>  
<A NAME = "6">
<H4> Sub-threshold voltage noise due to channel fluctutations in active
neuronal membranes </H4>

Voltage-gated  ion channels in  neuronal membranes  fluctuate randomly
between  different  conformational states  due  to thermal  agitation.
Fluctuations between conducting and non-conducting states give rise to
noisy membrane currents and sub-threshold voltage fluctuations and may
contribute   to  variability   in   spike  timing.    Here  we   study
sub-threshold voltage fluctuations  due to active voltage-gated sodium
and  potassium channels  as  predicted by  two  commonly used  kinetic
schemes: the Mainen et al.  (MJHS) kinetic scheme, which has been used
to  model dendritic channels  in cortical  neurons, and  the classical
Hodgkin-Huxley  (HH) kinetic  scheme  for the  squid  giant axon.   We
compute  the magnitudes, amplitude  distributions, and  power spectral
densities  of  the  voltage  noise in  isopotential  membrane  patches
predicted  by   these  kinetic  schemes.   For   both  schemes,  noise
magnitudes increase  rapidly with depolarization from  rest.  Noise is
larger  for smaller  patch areas  but is  smaller for  increased model
temperatures. We contrast the  results from Monte-Carlo simulations of
the stochastic non-linear kinetic schemes with analytical, closed-form
expressions   derived   using    passive   and   quasi-active   linear
approximations to the kinetic  schemes.  For all sub-threshold voltage
ranges, the  quasi-active linearized approximation  is accurate within
8%  and may  thus  be  used in  large-scale  simulations of  realistic
neuronal geometries.

<P><li>  
<A NAME = "7">
<H4> Detecting and  estimating signals over  noisy and unreliable
synapses:       Information-theoretic      analysis </H4>

Deciphering the neural code has been at the focus of recent efforts in
theoretical neuroscience.   The temporal precision  with which neurons
respond to  their inputs  has a  direct bearing on  the nature  of the
neuronal code.  Neuronal noise sources place fundamental limits on how
precisely  information  can  be  encoded  and  transmitted  in  neural
systems.  A  characterization of  the noise associated  with different
neuronal biophysical components (synapse,  dendrite, soma, axon and so
on)  is  needed  to  understand  the relationship  between  noise  and
information transfer in neural hardware.   Here we study the effect of
the  unreliable,  probabilistic  nature  of synaptic  transmission  on
information transfer in cortical neurons in the absence of interaction
among presynaptic  inputs.  We derive theoretical  lower-bounds on the
capacity of  a simple  model of a  cortical synapse under  the "signal
detection" and  "signal estimation" paradigms.   In signal estimation,
the signal  is assumed to  be encoded in  the mean firing rate  of the
presynaptic  neuron and the  objective is  to estimate  the continuous
input signal from the  postsynaptic voltage.  In signal detection, the
input is binary,  and the presence or absence  of a presynaptic action
potential  is  to  be  detected  from the  postsynaptic  voltage.  The
efficacy   of  information  transfer   in  synaptic   transmission  is
characterized   by  deriving  optimal   strategies  under   these  two
paradigms.   We find  that  single cortical  synapses cannot  transmit
information reliably  but redundancy obtained using a  small number of
multiple  synapses   leads  to   a  significant  improvement   in  the
information capacity of synaptic transmission.

<P><li>  
<A NAME = "8">
<H4> Channel noise in excitable neuronal membranes </H4>

Stochastic fluctuations of voltage-gated ion channels generate current
and voltage noise in neuronal membranes.  This noise may be a critical
determinant of  the efficacy  of information processing  within neural
systems.   Using Monte-Carlo  simulations, we  carry out  a systematic
investigation  of the  relationship between  channel kinetics  and the
resulting membrane voltage noise  using a stochastic Markov version of
the  Mainen-Sejnowski  model  of  dendritic excitability  in  cortical
neurons.  Our  simulations show that kinetic parameters  which lead to
an  increase in membrane  excitability (increasing  channel densities,
decreasing temperature) also  lead to an increase in  the magnitude of
the sub-threshold voltage noise.  Noise also increases as the membrane
is  depolarized  from  rest  towards threshold.   This  suggests  that
channel fluctuations may interfere with a neuron's ability to function
as an integrator of its  synaptic inputs and may limit the reliability
and precision of neural information processing.

<P><li>  
<A NAME = "9">
<H4> Variability  and coding  efficiency of noisy  neural spike
encoders </H4>

Encoding  synaptic  inputs  as  a  train of  action  potentials  is  a
fundamental function  of nerve cells.  Although  spike trains recorded
<i> in vivo </i> have been  shown to be highly variable, it is unclear
whether variability  in spike  timing represents faithful  encoding of
temporally  varying synaptic  inputs or  noise inherent  in  the spike
encoding  mechanism.    It  has   been  reported  that   spike  timing
variability is more pronounced for constant, unvarying inputs than for
inputs  with rich  temporal  structure.  This  could have  significant
implications for the nature  of neural coding, particularly if precise
timing of  spikes and  temporal synchrony between  neurons is  used to
represent information in the nervous system.
<P>
To study the potential functional role of spike timing variability, we
estimate  the  fraction  of  spike timing  variability  which  conveys
information about the input for two types of noisy spike encoders---an
integrate and fire  model with randomly chosen thresholds  and a model
of  a patch  of  neuronal membrane  containing  stochastic sodium  and
potassium  channels obeying Hodgkin-Huxley  kinetics.  The  quality of
signal encoding  is assessed by reconstructing the  input stimuli from
the output  spike trains using optimal  linear mean-square estimation.
A comparison of the estimation performance of noisy neuronal models of
spike generation enables us to  assess the impact of neuronal noise on
the efficacy  of neural coding.   The results for both  models suggest
that spike timing  variability reduces the ability of  spike trains to
encode rapid time-varying stimuli.  Moreover, contrary to expectations
based on earlier studies, we find that the noisy spike encoding models
encode  slowly-varying stimuli more  effectively than  rapidly varying
ones.

<P><li>  
<A NAME = "10">
<H4> Information-theoretic analysis of neuronal communication </H4>

One  of  the  most  fundamental  functions of  brains  is  to  process
information.  Whether  we are  engaged in tasks  like reading  a book,
listening to our favorite music station on radio, smelling a flower in
bloom or relishing our  favorite gourmet cuisine, we invariably employ
our brains to process the  information received through our senses and
create  a perception  of the  world around  us.  The  physical signals
incident on our  sensory organs, either in the  form of photon fluxes,
acoustic  vibrations,  or   plumes  of  chemical  concentrations,  are
transduced, represented and processed as electrical signals within our
brains.  One of the essential  inquiries in neuroscience is the nature
of  this representation  of information  in the  brain. This  is often
referred  to  as the  `neural  coding''  problem  which has  been  and
continues to  be the object of  a lot of  theoretical and experimental
scientific effort.
<P>
In most  theoretical approaches that address the  problem, nerve cells
are  characterized  empirically by  collection  of their  input-output
responses.   The  knowledge  of  constraints  imposed  on  information
processing due to biophysics  of the underlying biological hardware is
generally ignored.  This thesis reports  the outcome of our efforts to
combine techniques  from stochastic processes,  information theory and
single  neuron biophysics  to unravel  the neural  coding  problem. We
believe  that  a systematic  reductionist  analysis  which takes  into
account  the extant  noise  due to  biological  processes specific  to
neuronal  processing will provide  fundamental insights  overlooked in
earlier  approaches.   We  analytically  characterize the  sources  of
biological  noise associated  with  different stages  in the  neuronal
information pathway,  namely the synapse,  the dendritic tree  and the
spike-initiation zone  and employ information-theoretical  measures to
compute  the ability of  these components  to transmit  information in
specific  signal processing  tasks.  For  analytical  tractability, we
demonstrate  our results  using abstract  and  simplified mathematical
models. However, our approach can  be readily applied to realistic and
complicated  descriptions  of  single  neurons to  provide  a  greater
understanding of the role of noise in neuronal communication.


</OL>




