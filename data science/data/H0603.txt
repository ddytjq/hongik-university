ID=H0603
URL=http://www.lecb.ncifcrf.gov/~toms/paper/primer/latex/index.html
SIZE=33126
DATE=11/07/2002
TIME=17:06:32
DATASET=Biology
HTML=<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 98.1p1 release (March 2nd, 1998)
originally by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Information Theory Primer
With an
Appendix
on Logarithms Postscript version:
ftp://ftp.ncifcrf.gov/pub/delila/primer.ps web versions:
http://www.lecb.ncifcrf.gov/toms/paper/primer/
</TITLE>
<META NAME="description" CONTENT="Information Theory Primer
With an
Appendix
on Logarithms Postscript version:
ftp://ftp.ncifcrf.gov/pub/delila/primer.ps web versions:
http://www.lecb.ncifcrf.gov/toms/paper/primer/
">
<META NAME="keywords" CONTENT="paper">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<LINK REL="STYLESHEET" HREF="paper.css">
<LINK REL="next" HREF="node1.html">
</HEAD>
<BODY 
<body
bgcolor="#ffffff"
text="#000000"
link="#0000ff"
alink="#00ff00"
vlink="#888888"
>
>
<!--Navigation Panel-->
<A NAME="tex2html22"
 HREF="node1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="/~toms/latex2html.icons/next_motif.gif"></A> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="/~toms/latex2html.icons/up_motif_gr.gif"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="/~toms/latex2html.icons/previous_motif_gr.gif">   
<BR>
<B> Next:</B> <A NAME="tex2html23"
 HREF="node1.html">Bibliography</A>
<BR>
<BR>
<!--End of Navigation Panel-->

<P>
<font size=+2>

<P>

<H1 ALIGN="CENTER">
Information Theory Primer
<BR>
With an
<A HREF="node2.html#appendix">Appendix</A>
on Logarithms <BR>
Postscript version:
<A NAME="tex2html1"
 HREF="ftp://ftp.ncifcrf.gov/pub/delila/primer.ps">ftp://ftp.ncifcrf.gov/pub/delila/primer.ps</A>
<BR>
web versions:
<A NAME="tex2html2"
 HREF="http://www.lecb.ncifcrf.gov/<IMG
 WIDTH="20" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="$\sim$">toms/paper/primer/">http://www.lecb.ncifcrf.gov/<IMG
 WIDTH="20" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="$\sim$">toms/paper/primer/</A>
</H1>
<P ALIGN="CENTER"><STRONG>Thomas D. Schneider<A NAME="tex2html5"
 HREF="footnode.html#foot389"><SUP>1</SUP></A>
</STRONG></P>
<P ALIGN="CENTER"><STRONG>version = 2.51 of primer.tex 2002 February 10</STRONG></P>
<P ALIGN="LEFT"></P>

<P>
<BR>

<P>
This primer is written for molecular
biologists who are unfamiliar with information theory.
Its purpose is to introduce you to these ideas so that
you can understand how to apply them
to binding sites
[<A
 HREF="node1.html#Schneider1986">1</A>,<A
 HREF="node1.html#Schneider1988">2</A>,<A
 HREF="node1.html#Schneider1989">3</A>,<A
 HREF="node1.html#Schneider.Stephens1990">4</A>,<A
 HREF="node1.html#Herman.Schneider1992">5</A>,<A
 HREF="node1.html#Papp.helixrepa">6</A>,<A
 HREF="node1.html#Stephens.Schneider.Splice">7</A>,<A
 HREF="node1.html#Schneider.nano2">8</A>,<A
 HREF="node1.html#Rogan.Schneider.hmsh2.1995">9</A>].
Most of the material in this primer can also be found in introductory
texts on information theory.
Although
Shannon's original paper on the theory of information [<A
 HREF="node1.html#Shannon1948">10</A>]
is sometimes difficult to read, at other points
it is straight forward.  Skip the hard parts, and you
will find it enjoyable.
Pierce later published a popular book [<A
 HREF="node1.html#Pierce1980">11</A>]
which is a great introduction to information theory.
Other introductions are listed in reference [<A
 HREF="node1.html#Schneider1986">1</A>].
A workbook that you may find useful is reference [<A
 HREF="node1.html#Sacco1988">12</A>].
Shannon's complete collected works have been published [<A
 HREF="node1.html#Sloane.Wyner1993">13</A>].
Information about ordering this book is given in

<P>
<A NAME="tex2html7"
 HREF="http://www.lecb.ncifcrf.gov/<IMG
 WIDTH="20" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="$\sim$">toms/bionet.info-theory.faq.html#REFERENCES-Information_Theory">http://www.lecb.ncifcrf.gov/<IMG
 WIDTH="20" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="$\sim$">toms/bionet.info-theory.faq.html#REFERENCES-Information_Theory</A>
<P>
Other papers and documentation on programs can be found at

<P>
<A NAME="tex2html8"
 HREF="http://www.lecb.ncifcrf.gov/<IMG
 WIDTH="20" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="$\sim$">toms/">http://www.lecb.ncifcrf.gov/<IMG
 WIDTH="20" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="$\sim$">toms/</A>
<P>
<BR>
<BR>
Note:  If you have trouble getting through one or more steps in this primer,
please send email to me describing the exact place(s) that you had the
problem.  If it is appropriate, I will modify the text to smooth the path.  My
thanks go to the many people whose stubbed toes led to this version.

<P>

<BR>
<BR>
<B>Information and Uncertainty</B>

<P>
Information and uncertainty are technical terms that describe
any process that selects one or more objects from a set of objects.
We won't be dealing with the meaning or implications
of the information since nobody knows how to do that mathematically.
Suppose we have a device that can produce 3 symbols, A, B, or C.
As we wait for the next symbol, we are <EM>uncertain</EM>
as to which symbol it will produce.
Once a symbol appears and we see it,
our uncertainty <EM>decreases</EM>, and we remark that we have
received some <EM>information</EM>.  That is, information is a
decrease in uncertainty.
How should uncertainty be measured?
The simplest way would be to say that we have an
``uncertainty of 3 symbols''.  This would work well until
we begin to watch a second device at the same time, which,
let us imagine, produces symbols 1 and 2.
The second device gives us an ``uncertainty of 2 symbols''.
If we combine the devices into one device,
there are six possibilities,
A1, A2, B1, B2, C1, C2.
This device has an ``uncertainty of 6 symbols''.
This is not the way we usually think about information, for if we
receive two books, we would prefer to say that we received
twice as much information than from one book.
That is, we would like our measure to be additive.

<P>
<BR>
<BR>
It's easy to do this if we first take the logarithm of
the number of possible symbols
because then we can add the logarithms instead of multiplying the number
of symbols.
In our example, the first device makes us uncertain by <IMG
 WIDTH="53" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.gif"
 ALT="$\log(3)$">,
the second by <IMG
 WIDTH="53" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.gif"
 ALT="$\log(2)$">
and the combined device by 
<!-- MATH: $\log(3)+\log(2)=\log(6)$ -->
<IMG
 WIDTH="197" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img4.gif"
 ALT="$\log(3)+\log(2)=\log(6)$">.
The base of the logarithm determines the units.
When we use the base 2 the units are in bits
(base 10 gives digits and the base of the natural logarithms, <I>e</I>,
gives nits).
Thus if a device produces one symbol,
we are uncertain by 
<!-- MATH: $\log_2 1 = 0$ -->
<IMG
 WIDTH="84" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img5.gif"
 ALT="$\log_2 1 = 0$">
bits, and we have no uncertainty about
what the device will do next.
If it produces two symbols our uncertainty would be

<!-- MATH: $\log_2 2 = 1$ -->
<IMG
 WIDTH="84" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img6.gif"
 ALT="$\log_2 2 = 1$">
bit.
In reading an mRNA, if the ribosome encounters any one of 4
equally likely bases,
then the uncertainty is 2 bits.

<P>
<BR>
<BR>
So far, our formula for uncertainty is 
<!-- MATH: $\log_2 ( M )$ -->
<IMG
 WIDTH="71" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.gif"
 ALT="$\log_2 ( M )$">,
with <I>M</I> being the
number of symbols.  The next step is to extend the formula
so it can handle cases where the symbols are not equally likely.
For example, if there are 3 possible symbols, but one of them never
appears, then our uncertainty is 1 bit.  If the third symbol appears
rarely relative to the other two symbols,
then our uncertainty should be larger
than 1 bit, but not as high as <IMG
 WIDTH="60" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.gif"
 ALT="$\log_2(3)$">
bits.
Let's begin by rearranging the formula like this:
<BR>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{eqnarray}
\log_2 ( M ) & = & - \log_2 ( M^{-1} ) \\
& = & - \log_2 ( \frac{1}{M} ) \nonumber \\
             & = & - \log_2 ( P ) \nonumber
\end{eqnarray} -->

<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="71" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.gif"
 ALT="$\displaystyle \log_2 ( M )$"></TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="107" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img10.gif"
 ALT="$\displaystyle - \log_2 ( M^{-1} )$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(1)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="93" HEIGHT="58" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.gif"
 ALT="$\displaystyle - \log_2 ( \frac{1}{M} )$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="84" HEIGHT="37" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.gif"
 ALT="$\displaystyle - \log_2 ( P )$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
so that <I>P</I> = 1/<I>M</I> is the probability that any symbol appears.
(If you don't remember this trick of `pulling the sign out', recall
that 
<!-- MATH: $\log M^b = b \log M$ -->
<IMG
 WIDTH="142" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.gif"
 ALT="$\log M^b = b \log M$">
and let <I>b</I> = -1.)

<P>
<BR>
<BR>
Now let's generalize this
for various probabilities of the symbols, <I>P</I><SUB><I>i</I></SUB>, so that
the probabilities sum to 1:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
\sum_{i = 1}^M P_i = 1 .
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="81" HEIGHT="59"
 SRC="img14.gif"
 ALT="\begin{displaymath}\sum_{i = 1}^M P_i = 1 .
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(2)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>(Recall that the <IMG
 WIDTH="22" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img15.gif"
 ALT="$\sum$">
symbol means to add the <I>P</I><SUB><I>i</I></SUB> together, for
<I>i</I> starting at 1 and ending at <I>M</I>.)
The surprise that
we get when we see the i<SUP><I>th</I></SUP> kind of symbol
was called the ``surprisal'' by Tribus [<A
 HREF="node1.html#Tribus1961">14</A>]
and is defined by analogy with 
<!-- MATH: $- \log_2 P$ -->
<IMG
 WIDTH="72" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img16.gif"
 ALT="$- \log_2 P$">
to be:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
u_i = - \log_2 ( P_i ) .
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="127" HEIGHT="33"
 SRC="img17.gif"
 ALT="\begin{displaymath}u_i = - \log_2 ( P_i ) .
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(3)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
For example, if <I>P</I><SUB><I>i</I></SUB> approaches 0, then we will be
<EM>very surprised</EM> to see the i<SUP><I>th</I></SUP> symbol (since
it should almost never appear), and the formula
says <I>u</I><SUB><I>i</I></SUB> approaches <IMG
 WIDTH="24" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img18.gif"
 ALT="$\infty$">.
On the other hand, if <I>P</I><SUB><I>i</I></SUB>=1,
then we won't be surprised at all
to see the i<SUP><I>th</I></SUP> symbol
(because it should always appear) and <I>u</I><SUB><I>i</I></SUB> = 0.

<P>
<BR>
<BR>
Uncertainty is the <EM>average surprisal</EM> for the infinite
string of symbols produced by our device.
For the moment, let's find the average for a string of only
<I>N</I> symbols.
Suppose that the i<SUP><I>th</I></SUP> type symbol appears <I>N</I><SUB><I>i</I></SUB> times
so that
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
N = \sum_{i = 1}^M N_i .
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="93" HEIGHT="59"
 SRC="img19.gif"
 ALT="\begin{displaymath}N = \sum_{i = 1}^M N_i .
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(4)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
There will be <I>N</I><SUB><I>i</I></SUB> cases where we have surprisal <I>u</I><SUB><I>i</I></SUB>.
The average surprisal for the N symbols is:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
\frac
{\sum_{i = 1}^M N_i u_i}
     {\sum_{i = 1}^M N_i} .
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="85" HEIGHT="55"
 SRC="img20.gif"
 ALT="\begin{displaymath}\frac
{\sum_{i = 1}^M N_i u_i}
{\sum_{i = 1}^M N_i} .
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(5)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
By substituting <I>N</I> for the denominator and bringing it inside the upper
sum, we obtain:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
{\sum_{i = 1}^M {\frac{N_i}{N}} u_i}
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="65" HEIGHT="59"
 SRC="img21.gif"
 ALT="\begin{displaymath}{\sum_{i = 1}^M {\frac{N_i}{N}} u_i}
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(6)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
If we do this measure for an infinite string of symbols, then
the frequency <I>N</I><SUB><I>i</I></SUB> / <I>N</I> becomes
<I>P</I><SUB><I>i</I></SUB>, the probability of the i<SUP><I>th</I></SUP> symbol.
Making this substitution, we see that our
average surprisal (H) would be:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
H =  \sum_{i = 1}^M P_i u_i .
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="106" HEIGHT="59"
 SRC="img22.gif"
 ALT="\begin{displaymath}H = \sum_{i = 1}^M P_i u_i .
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(7)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
<P>
<BR>
<BR>
Finally, by substituting for <I>u</I><SUB><I>i</I></SUB>, we get
Shannon's famous general formula for uncertainty:

<P>
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
\framebox{$\displaystyle
H = - \sum_{i = 1}^M P_i \log_2 P_i
\;\;\;\;\;\mbox{(bits per symbol).}
$ }
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="def.famous">&#160;</A><IMG
 WIDTH="350" HEIGHT="69"
 SRC="img23.gif"
 ALT="\begin{displaymath}\framebox{$\displaystyle
H = - \sum_{i = 1}^M P_i \log_2 P_i
\;\;\;\;\;\mbox{(bits per symbol).}
$ }\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(8)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
<P>
<BR>
<BR>
Shannon got to this formula by a much more rigorous route
than we did, by setting
down several desirable properties for uncertainty,
and then deriving the function.
Hopefully the route we just followed gives you a feeling for
how the formula works.

<P>

<P>
To see how the H function looks, we can plot it for the case of
two symbols.
This is shown below<A NAME="tex2html9"
 HREF="footnode.html#foot368"><SUP>2</SUP></A>:

<P>
<BR>
<A NAME="figure99"
 HREF="img24.gif"><IMG
 WIDTH="242" HEIGHT="271" SRC="Timg24.gif"
 ALT="\begin{figure}%
\vspace{10cm}
\special{psfile=''fig/hgraph.ps''
hoffset=0 voffset=-130
hscale=100 vscale=100
angle=0}
\end{figure}"></A>
<BR> 

<P>
Notice that the curve is symmetrical, and rises to a maximum when the two
symbols are equally likely (probability = 0.5).
It falls towards zero whenever one of the symbols becomes dominant
at the expense of the other symbol.

<P>
<BR>
<BR>
As an instructive exercise, suppose that all the symbols are equally likely.
What does the formula for H (equation (<A HREF="paper.html#def.famous">8</A>)) reduce to?
You may want to try this yourself before reading on.

<P>
<DIV ALIGN="CENTER">
*********************************
</DIV>

<P>
Equally likely means that <I>P</I><SUB><I>i</I></SUB> = 1/<I>M</I>, so if we substitute
this into the uncertainty equation we get:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
H_{equiprobable} =  - \sum_{i = 1}^M \frac{1}{M} \log_2 \frac{1}{M}
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="244" HEIGHT="59"
 SRC="img25.gif"
 ALT="\begin{displaymath}H_{equiprobable} = - \sum_{i = 1}^M \frac{1}{M} \log_2 \frac{1}{M}
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(9)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
Since <I>M</I> is not a function of <I>i</I>,
we can pull it out of the sum:
<BR>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{eqnarray}
H_{equiprobable} & = & - ( \frac{1}{M} \log_2 \frac{1}{M} )
\sum_{i = 1}^M 1 \\
  & = & - \left( \frac{1}{M} \log_2 {\frac{1}{M}} \right) M \nonumber \\
  & = & - \log_2 \frac{1}{M} \\
  & = &   \log_2 M \nonumber
\end{eqnarray} -->

<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><I>H</I><SUB><I>equiprobable</I></SUB></TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="159" HEIGHT="70" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.gif"
 ALT="$\displaystyle - ( \frac{1}{M} \log_2 \frac{1}{M} )
\sum_{i = 1}^M 1$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(10)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="155" HEIGHT="58" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.gif"
 ALT="$\displaystyle - \left( \frac{1}{M} \log_2 {\frac{1}{M}} \right) M$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="82" HEIGHT="58" ALIGN="MIDDLE" BORDER="0"
 SRC="img28.gif"
 ALT="$\displaystyle - \log_2 \frac{1}{M}$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(11)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="60" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img29.gif"
 ALT="$\displaystyle \log_2 M$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
which is the simple equation we started with.
It can be shown that for a given number of symbols (ie., <I>M</I> is fixed)
the uncertainty
<I>H</I> has its largest value only when the symbols are equally probable.
For example, an unbiased coin is harder to guess than
a biased coin.
As another exercise, what is the uncertainty when there are
10 symbols and only one of them appears?
(clue: 
<!-- MATH: $\displaystyle \lim_{p \rightarrow 0} p \log p = 0$ -->
<IMG
 WIDTH="119" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.gif"
 ALT="$\displaystyle \lim_{p \rightarrow 0} p \log p = 0$">
by setting <I>p</I> = 1/<I>M</I>and using l'H&#244;pital's rule, so 
<!-- MATH: $0 \log_2  0 = 0$ -->
<IMG
 WIDTH="96" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img31.gif"
 ALT="$0 \log_2 0 = 0$">.)

<P>
<BR>
<BR>
What does it mean to say that a signal has 1.75 bits per symbol?
It means that we can convert the original signal into a string of
1's and 0's (binary digits),
so that <EM>on the average</EM> there are 1.75 binary digits for
every symbol in the original signal.
Some symbols will need more binary digits (the rare ones)
and others will need fewer (the common ones).
Here's an example.  Suppose we have <I>M</I> = 4 symbols:
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
A
\;\;\;\;\;
C
\;\;\;\;\;
  G
\;\;\;\;\;
  T
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="137" HEIGHT="28"
 SRC="img32.gif"
 ALT="\begin{displaymath}A
\;\;\;\;\;
C
\;\;\;\;\;
G
\;\;\;\;\;
T
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(12)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
with probabilities (<I>P</I><SUB><I>i</I></SUB>):
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
P_A =  \frac{1}{2} ,
\;\;\;\;\;
P_C =  \frac{1}{4} ,
\;\;\;\;\;
  P_G =  \frac{1}{8} ,
\;\;\;\;\;
  P_T =  \frac{1}{8} ,
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="354" HEIGHT="45"
 SRC="img33.gif"
 ALT="\begin{displaymath}P_A = \frac{1}{2} ,
\;\;\;\;\;
P_C = \frac{1}{4} ,
\;\;\;\;\;
P_G = \frac{1}{8} ,
\;\;\;\;\;
P_T = \frac{1}{8} ,
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(13)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
which have surprisals (
<!-- MATH: $- \log_2 P_i$ -->
<IMG
 WIDTH="75" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img34.gif"
 ALT="$- \log_2 P_i$">):
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
u_A =  1  \mbox{ bit},
\;\;\;\;\;
u_C =  2  \mbox{ bits},
\;\;\;\;\;
  u_G =  3  \mbox{ bits},
\;\;\;\;\;
  u_T =  3  \mbox{ bits},
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="472" HEIGHT="32"
 SRC="img35.gif"
 ALT="\begin{displaymath}u_A = 1 \mbox{ bit},
\;\;\;\;\;
u_C = 2 \mbox{ bits},
\;\;\;\;\;
u_G = 3 \mbox{ bits},
\;\;\;\;\;
u_T = 3 \mbox{ bits},
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(14)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
so the uncertainty is
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
H =
\frac{1}{2} \cdot 1  + 
\frac{1}{4} \cdot 2  + 
\frac{1}{8} \cdot 3  + 
\frac{1}{8} \cdot 3  = 
1.75
\mbox{ (bits per symbol)} .

\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="466" HEIGHT="45"
 SRC="img36.gif"
 ALT="\begin{displaymath}H =
\frac{1}{2} \cdot 1 +
\frac{1}{4} \cdot 2 +
\frac{1}{8...
...t 3 +
\frac{1}{8} \cdot 3 =
1.75
\mbox{ (bits per symbol)} .
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(15)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
Let's recode this so that the number
of binary digits equals the surprisal:
<BR>
<DIV ALIGN="CENTER"><A NAME="eqn.fanocode">&#160;</A>
<!-- MATH: \begin{eqnarray}
A & = & 1 \nonumber \\
C & = & 01 \nonumber \\
  G & = & 000 \nonumber \\
  T & = & 001
\end{eqnarray} -->

<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><I>A</I></TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP>1</TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><I>C</I></TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP>01</TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><I>G</I></TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP>000</TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><I>T</I></TD>
<TD ALIGN="CENTER" NOWRAP>=</TD>
<TD ALIGN="LEFT" NOWRAP>001</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(16)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
so the string
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
ACATGAAC
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP>
<I>ACATGAAC</I>
</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(17)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
which has frequencies the same as the probabilities defined above,
is coded as
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
10110010001101 .
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP>
10110010001101 .
</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(18)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
14 binary digits were used to code for 8 symbols, so the
average is 14/8 = 1.75 binary digits per symbol.
This is called a Fano code.
Fano codes have the property
that you can decode them without needing spaces between symbols.
Usually one needs to know the ``reading frame'', but in
this example one can figure it out.
In this particular coding (equations (<A HREF="paper.html#eqn.fanocode">16</A>)),
the first binary digit distinguishes between
the set containing <I>A</I>, (which we symbolize as <I>A</I>)
and the set <I>C</I>, <I>G</I>, <I>T</I>, which are equally likely.
The second digit, used if the first digit is 0,
distinguishes <I>C</I> from <I>G</I>, <I>T</I>.
The final digit distinguishes <I>G</I> from <I>T</I>.
Because each choice is equally likely (in our original definition of the
probabilities of the symbols),
every binary digit in this code carries 1 bit of information.
<EM>Beware!</EM>
This won't always be true.
A binary digit can supply 1 bit <EM>only</EM>
if the two sets represented by the digit are equally likely
(as rigged for this example).
If they are not equally likely, one binary digit supplies
less than one bit.  (Recall that H is at a
maximum for equally likely probabilities.)
So if the probabilities were
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
P_A =  \frac{1}{2},
\;\;\;\;\;
P_C =  \frac{1}{6},
\;\;\;\;\;
  P_G =  \frac{1}{6},
\;\;\;\;\;
  P_T =  \frac{1}{6},
\end{equation} -->

<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="354" HEIGHT="45"
 SRC="img37.gif"
 ALT="\begin{displaymath}P_A = \frac{1}{2},
\;\;\;\;\;
P_C = \frac{1}{6},
\;\;\;\;\;
P_G = \frac{1}{6},
\;\;\;\;\;
P_T = \frac{1}{6},
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(19)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
there is no way to assign a (finite) code so that
each binary digit has the value of one bit
(by using larger blocks of symbols, one can approach it).
In the rigged example, there is no way to use fewer
than 1.75 binary digits per symbol,
but we could be wasteful and use extra
digits to represent the signal.
The Fano code does reasonably well
by splitting the symbols into successive
groups that are equally likely to occur; you can read more about it
in texts on information theory.
The uncertainty measure tells us what could be done ideally, and
so tells us what is impossible.  For example, the signal with 1.75 bits
per symbol could not be coded using only 1 binary digit per symbol.

<P>
<BR>
<BR>
<B>Tying the Ideas Together</B>

<P>
In the beginning of this primer we took information to be a decrease
in uncertainty.
Now that we have a general formula for uncertainty,
(<A HREF="paper.html#def.famous">8</A>), we can express information using this formula.
Suppose that a computer contains some information in its memory.
If we were to look at individual flip-flops, we would have an uncertainty

<!-- MATH: $H_{before}$ -->
<I>H</I><SUB><I>before</I></SUB> bits per flip-flop.
Suppose we now clear part of
the computer's memory (by setting the values there to zero),
so that there is a new uncertainty, smaller
than the previous one: <I>H</I><SUB><I>after</I></SUB>.  Then the computer memory has lost
an average of
<BR><P></P>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{equation}
R =  H_{before} - H_{after}
\end{equation} -->
<A NAME="def.decrease">&#160;</A>
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP>
<I>R</I> =  <I>H</I><SUB><I>before</I></SUB> - <I>H</I><SUB><I>after</I></SUB>
</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(20)</TD></TR>
</TABLE>
</DIV>
<BR CLEAR="ALL"><P></P>
bits of information per flip-flop.
If the computer was completely cleared, then 
<!-- MATH: $H_{after} = 0$ -->
<I>H</I><SUB><I>after</I></SUB> = 0 and

<!-- MATH: $R = H_{before}$ -->
<I>R</I> = <I>H</I><SUB><I>before</I></SUB>.

<P>
<BR>
<BR>
Now consider a teletype receiving characters over a phone line.
If there were no noise in the phone line and no other source
of errors, the teletype would print the text perfectly.
With noise, there is some uncertainty about whether a character printed
is really the right one.  So before a character is printed, the
teletype must be prepared for any of the letters, and this prepared state
has uncertainty 
<!-- MATH: $H_{before}$ -->
<I>H</I><SUB><I>before</I></SUB>,
while after each character has been received there is still some uncertainty,
<I>H</I><SUB><I>after</I></SUB>.
This uncertainty
is based on the probability that the symbol that came through 
is not equal to the symbol that was sent, and it measures the
amount of noise.

<P>
<BR>
<BR>
Shannon gave an example of this in section 12 of [<A
 HREF="node1.html#Shannon1948">10</A>]
(pages 33-34 of [<A
 HREF="node1.html#Sloane.Wyner1993">13</A>]).
A system with two equally likely symbols transmitting every second
would send at a rate of 1 bit per second without errors.
Suppose that the probability that a 0 is received when a 0 is sent is
0.99 and the probability of a 1 received is 0.01.
``These figures are reversed if a 1 is received.''
Then the uncertainty after receiving a symbol is

<!-- MATH: $H_{after} =
- 0.99 \log_2 0.99
- 0.01 \log_2 0.01
= 0.081$ -->
<IMG
 WIDTH="394" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img38.gif"
 ALT="$H_{after} =
- 0.99 \log_2 0.99
- 0.01 \log_2 0.01
= 0.081$">,
so that the actual rate of transmission is

<!-- MATH: $R = 1 - 0.081 = 0.919$ -->
<I>R</I> = 1 - 0.081 = 0.919 bits per second.<A NAME="tex2html10"
 HREF="footnode.html#foot370"><SUP>3</SUP></A>
The amount of
information that gets through is given by the decrease
in uncertainty, equation (<A HREF="paper.html#def.decrease">20</A>).

<P>
<BR>
<BR>
Unfortunately
many people have made errors because they did not keep this point clear.
The errors occur because people implicitly
assume that there is no noise in the communication.
When there is no noise,

<!-- MATH: $R = H_{before}$ -->
<I>R</I> = <I>H</I><SUB><I>before</I></SUB>, as with the completely cleared computer memory.
That is <EM>if there is no noise, the amount of information communicated is
equal to the uncertainty before communication.</EM>
When there is noise and someone assumes that there isn't any,
this leads to all kinds of confusing philosophies.
One must always account for noise.

<P>
<BR>
<BR>
<B>One Final Subtle Point.</B>
In the previous section
you may have found it odd that I used the word ``flip-flop''.
This is because I was intentionally avoiding the use of the
word ``bit''.  The reason is that there are two meanings to this word,
as we mentioned before while discussing Fano coding,
and it is best to keep them distinct.
Here are the two meanings for the word ``bit'':
<DL COMPACT>
<DT>1.
<DD>A binary digit, 0 or 1.  This can only be an integer.
These `bits' are the individual pieces of data in computers.
<DT>2.
<DD>A <EM>measure</EM> of uncertainty, <I>H</I>, or information <I>R</I>.
This can be any real number because it is an average.
It's the measure that Shannon used to discuss communication systems.
</DL>
<P>

<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS">&#160;</A>
<UL>
<LI><A NAME="tex2html24"
 HREF="node1.html">Bibliography</A>
<LI><A NAME="tex2html25"
 HREF="node2.html"><FONT SIZE="+2"><B>APPENDIX: A Tutorial On Logarithms</B></FONT>
</A>
<LI><A NAME="tex2html26"
 HREF="node3.html">About this document ... </A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html22"
 HREF="node1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="/~toms/latex2html.icons/next_motif.gif"></A> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="/~toms/latex2html.icons/up_motif_gr.gif"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="/~toms/latex2html.icons/previous_motif_gr.gif">   
<BR>
<B> Next:</B> <A NAME="tex2html23"
 HREF="node1.html">Bibliography</A>
<!--End of Navigation Panel-->
<ADDRESS>
<I>Tom Schneider</I>
<BR><I>2002-02-10</I>
</ADDRESS>
</BODY>
</HTML>

